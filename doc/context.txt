
1. Intro

Our goal here is to identify techniques for implementing conveyances while using the C
language.  C is chosen because it is more portable than assembly. Later we will create
enhancements to C, or possibly a new language, which will have direct support for these
techniques.

Note we are using data 'pads' for passing arguments.  Data pads are placed in union and
included into a function, typically into main. When included in main they are never popped
until the program returns, but this sort of allocation differs from that of static
allocation because we get independent copies in different threads. Perhaps this should be
called 'stack static' allocation or some such. Nothing prevents a programmer for
statically allocating a pad, but that is not the design intention.  

In C there is an outermost file wide lexical scope.  Variables and functions may be defined
at file scope scope. Such variables are statically allocated.

Within a function definition matching curly braces may be used to created nested levels of
lexical scope.  Programs and statements within a given nesting level may see variables declared at
outer nesting levels, but not those at deeper levels.  By default such variables will
be allocated on the stack.

The C standard only allows for functions to be declared at file scope. Functions are
statically allocated, i.e. have fixed locations in memory.  There is an extern keyword for
declaring functions who's definitions were given in other source files. According to the C
standard, functions may not be declared at nested scope levels.  The compiler will issue
an error and refuse to compile such funcitons.

In the definition of the TM library we made use of the technique of passing in locally
defined, often anonymous, continuation functions as arguments.  In C we may pass in
function pointers as arguments to functions, which is probably how the lambda passing
is implemented anyway, however we may not locally define such functions due to the restriction
on nesting levels.  Furthermore locally defined functions in Lisp may take advantage
of variables what are within lexical scope without passing them in as arguments.  This has
proven to be a natural thing to do when defining a function at a lower nesting level of
lexical scope..

Gcc provides an extension for defining nested functions, and these nested functions may make
use of variables within lexical scope.  These functions may not be anonymous but that is
only a detail.  This nicely facilitates the programming style of using continuations. 

However, we are using conveyances rather than functions. We can emulated nested conveyances
using conveyances encapsulated within functions, as described further below, but we
would rather have a direct implementation of nested conveyances.

We are using a sort of hack to implement conveyances in C.  Accordingly we use a label
followed by an open brace that defines a nested lexical scope.  Then within the conveyance
we follow a continuation by using the 'continue_from' macro.  This hack is not that much
different than how functions are implemented.  After all, all this stuff has to be turned
into assembly language, and that typically has labels along with branch, and call instructions.
Whereas a C function will have a label and be invoked through a call, our conveyance will
have a label and be continued to with an branch instruction.

There is a limitation in the C standard where labels are always taken to be at function
scope no matter where they occur in the lexical scope hierarchy.  However, thankfully,
Gcc has another extension that lets us put labels into th lexcial hierarchy by declaring
them after an opening brace.

The second problem we need to solve is that of sharing variables at an outer lexical scope
with the conveyance that is defined within that scope. This is a challenge because conveyances
do not make use of the stack.  Where as nested functions and lambads may just reach up
in the stack to find such variables, the local variables of a conveyance are on a data pad
that is within an union with other data pads, so by the time we reach a nested function the
out scope variables, so to speak, may well have been overwritten.

2. Analysis of the outerscope variable passing problem

We have a situation like the following:

   .--c1
   |  |
   |  [c2]
    \ |   \
      c3 
      | \

Here c1 is our outer lexical scope conveyance.  c3 is our nested conveyeance that has been
defined within that scope.  [c2] is a continuation call into conveyance c2.  Its
definition lies elswhere.  Here c2 continues into c3 which is again withing the c1 lexical
scopes.  It is common that a conveyance will have multiple continuations, so c3 might just
be one among a number of possibilities.  c3 will also continue off somewhere, as all
conveyances that do not exit the program will have at least one continuation.

Consider this example, here TM2x·dealloc_heap is the outer conveyance, c1.  We will call
TM2x·destruct, which corresponds to [c2] in our diagram, and then TM2x·destruct will
continue on to the nested conveyance called 'success, which corresponds to c3.

    TM2x·dealloc_heap:{

      // Swaps the args and locals pads.
      Conveyance·swap(); 

      // Makes lc0 an alias for Conveyance·Locals_pt->dealloc_heap.
      register struct TM2x·dealloc_heap0 *lc0 = &Conveyance·Locals_pt->dealloc_heap;

      // Puts arguments for TM2x·destruct on to the arguments pad
      struct TM2x·destruct0 *ar = &Conveyance·Args_pt->TM2x·destruct;
      ar->tape = lc0->tape;
      ar->nominal = &&success; // this is a reference to our nested conveyance
      continue_from TM2x·destruct;

      // a nested conveyance definition
      success:{
        free( <message>->tape);
        continue_from  <message>->nominal;
        cend; 
      }

      cend; // it is an error to reach cend
    }

Our c3 analog, 'TM2x·dealloc_heap', accesses variables at the 'outer scope', namely
lc0->tape and lc0->nominal.  lc0->nominal is a pointer to the nominal continuation for
TM2x·dealloc_heap, the outer conveyance.  That continuation is set to 'success'.  We
may think of the values lc0->tape and lc->nominal as messages that are sent from c1 to
c3.

3. Messaage relaying

If the programmer has control over c1, c2, and c3, then he or she may design the code so
that c2 relays the message for c1 by making it an argument of c2.  However, if the
programmer desires, or is comppelled, to keep the definitions of c1 and c3 as separate
code maintinance tasks, a different solution will be needed.  A solution that does not
depend up on how c2 uses its data pad.

In general message relaying is a pain for a human programmer because he or she has
to keep track of all the call dependencies.

3. Context on the stack.

Upon being called each function is given a stack frame.  Upon return from
the function the stack frame is popped.  Thus local variables are allocated and
cleaned up.

Our conveyances do not make use of the stack, but they do make use of nested lexical
scope. According to lexical scoping rules in C a variable may only be used after it is
declared, and may not be referenced after the next closing brace at that scope level.
Hence, a programmer might conclude that this structure is parallleled in the generated
code.  I.e. conclude that when the compiler would insert code to push variables declared
within a lexcial scope at the pointer where the opening brace occurs, or even where the
local variable is declared, and then would generate code to pop such local variables off
the stack at the place where the matching closing brace occurs in the source.  Such a
compilation strategy would also have to add pop instructions before branches that left the
code generated from said lexcial scope.

If C compilers generated code in this manner, i.e. code that dynamically parralleled the
static lexical scope structures, then it would be straightforward for c1 to declare
variables to be sent to c3 in the normal fashion, and it would work just like it works for
functions, such variables would get allocated with a push onto the stack upon entering the
conveyance, and then be deallocated when by being popped off when leaving it, because our
conveyances always start with an opening brace and end with a closing one.

However, C compilers are not obligated by the C standard to create parallel structures
between lexical scopes and generated code. The constraint is that the compiler must
respect the programmers view of the C standard, thus maintain the appearence of variables
only being available within their correpsonding lexical scopes.

In an example I coded up in the the try directory everything gets reserved upon the
entrance to main, and all get popped off when main returns.  This happens independent of
the optimization level. With this approach the compiler need not generate pops upon
leaving a lexical scope.  However this simplification comes at a cost, in that if, due to
the vagaries of the computation, we never use a particular lexical scope, the variables
declared in that scope still take up memory.  I used this effect to cause the program
shown in the try directory to crash due to not having enough stack space, though it
actually only needs a few bytes of space.

The approach that Gcc is using will cause all of the shared context variable in all
of the conveyances that are encapsulated by a function to be allocated together on
that functions frame, independent of the path of continuation through the conveyances.
We end up with an 'all contexts separate and always allocated' scenario.

As a side note, C++ compilers do guarantee that object destructors will be called when
leaving a lexical scope, even if that departure is due to a 'goto'.  But this does not
help us here, because even if destructors are called, all of our contexts still might end
up allocated on the stack without ever being deallocated until the encapsulating function
returns.  We need a deallocation guarantee, not a destructor call guarantee.

We could fource the the behavior we want by using some inline asm functions and explicit
calls to them. It is interesting that we are talking about push and pop control for
contexts used in messenging, not arguments or local variables.

An interesting aspect of using the stack for sharing context is that the stack grows and
shrinks with context usage. Even in a structure where conveyance recursively continues
into itself, the stack will not accumulate in size. A reasonable bound may be placed on the
stack size through static analysis of the code.  I.e. it is the code structure rather
than the run time behavior which determines stack size. Of course a program can still have
memory complexity at run time, but that will be expressed through heap usage rather than
stack usage.

Rather than using the system stack we might implement our own on the heap.  An expanding
array would be perfect for that .. but that is what we are implementing right now, so
that would be circular problem.  Another complication is that when creating a second thread
we would have to give the new thread a copy of the stack.  We would not have the advantage
of the virtual memory system to add performance to this.

4. Context Pads

We add a data pad in analogy to our arguments pad.  Both c1 and c3 know about this pad and
may use it to relay a message. 


In analogy to the arguments data pad, all such contexts used throughout our code that can not be
in simultaneous used may be placed in union, just as we do with the argument pads.  Ideally we
would end up with N such context pads, where N is the static nesting levels of message passing we
find in our source code.

We would expect that code modules would have their own context pads.  That way static analysis
can be limited to a module.

Static analysis will not always be sufficient.  Say for example a conveyance takes a continuation
argument where the value of that argument is not known at run time.  In which case would have
have to make a worse case assumption that such an argument might be any one of a set of values.
This might mean that a given function would have to have an independent context from every
one of those other conveyances.

5. Comparison

We have discussed three context sharing techniques:

    1. relays
    2. stack
    3. context  pads

Among them the stack approach is the only one that does not require the programmer to be
aware of the behavior of the code to get the logic of the program correct.  In some ways
tt resembles the context pad approach where the pads are allocated at run time on the
heap instead of being allocated 'stack static'.

The relay where all the conveyances are sitting together in the same file gives the
compiler the most chance for optimization.

Context pads have the advantage that we do not use the stack, and can not get stack
overflow errors at run time for single threaded execution.  In a multiple threading
environment the stack space that is used is duplicated for each thread, so in this sense,
if the number of threads depends upon the computation, the stack can still grow by an
amount that depends on the computation. Also, if the program makes use of the heap,
there can still be memory resource shortages.  Still there is something to be said for
having only one expanding memory problem to analyze instead of two.

Because the expanding array is fundamental, the code is pretty simple and easily
analyzable, and as it is a library the library developer has full control of the code
base, so I am going to use relaying and context pads that are dedicated to the library.












